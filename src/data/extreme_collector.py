""" Extreme data collection module for maximum accuracy. """ import pandas as pd import numpy as np import yfinance as yf import requests import time import concurrent.futures from typing import List, Dict, Optional from datetime import datetime, timedelta import ta from loguru import logger import alpha_vantage from alpha_vantage.timeseries import TimeSeries from alpha_vantage.fundamentaldata import FundamentalData from alpha_vantage.techindicators import TechIndicators import quandl import fredapi import newsapi import tweepy class ExtremeDataCollector: """Collects massive amounts of market data from multiple sources.""" def __init__(self, config_path: str = "config/config.yaml"): """Initialize with all API keys.""" with open(config_path, 'r') as file: self.config = yaml.safe_load(file) # Initialize all data sources self.alpha_vantage_key = self.config['api_keys']['alpha_vantage'] self.fred_key = self.config['api_keys'].get('fred', '') self.quandl_key = self.config['api_keys'].get('quandl', '') self.news_key = self.config['api_keys']['news'] # Initialize APIs self.ts = TimeSeries(key=self.alpha_vantage_key, output_format='pandas') self.fd = FundamentalData(key=self.alpha_vantage_key, output_format='pandas') self.ti = TechIndicators(key=self.alpha_vantage_key, output_format='pandas') if self.fred_key: self.fred = fredapi.Fred(api_key=self.fred_key) if self.quandl_key: quandl.ApiConfig.api_key = self.quandl_key def get_massive_stock_data(self, symbol: str, years: int = 10) -> pd.DataFrame: """Get massive historical data.""" try: logger.info(f" Collecting MASSIVE data for {symbol} ({years} years)") # Primary source: Yahoo Finance (daily data) ticker = yf.Ticker(symbol) # Get maximum historical data hist_data = ticker.history(period=f"{years}y", interval="1d") # Get intraday data for recent period (1 month) intraday_1h = ticker.history(period="1mo", interval="1h") intraday_15m = ticker.history(period="7d", interval="15m") intraday_5m = ticker.history(period="1d", interval="5m") # Aggregate intraday to daily daily_from_1h = self._aggregate_intraday_to_daily(intraday_1h) daily_from_15m = self._aggregate_intraday_to_daily(intraday_15m) daily_from_5m = self._aggregate_intraday_to_daily(intraday_5m) # Combine with extra intraday features combined_data = self._combine_timeframes(hist_data, daily_from_1h, daily_from_15m, daily_from_5m) logger.info(f" Collected {len(combined_data)} records with multi-timeframe data") return combined_data except Exception as e: logger.error(f"Error collecting massive stock data: {e}") return pd.DataFrame() def get_fundamental_data(self, symbol: str) -> Dict: """Get comprehensive fundamental data.""" try: logger.info(f" Getting fundamental data for {symbol}") fundamentals = {} # Company overview overview, _ = self.fd.get_company_overview(symbol) fundamentals['overview'] = overview # Income statement income_annual, _ = self.fd.get_income_statement_annual(symbol) income_quarterly, _ = self.fd.get_income_statement_quarterly(symbol) fundamentals['income_annual'] = income_annual fundamentals['income_quarterly'] = income_quarterly # Balance sheet balance_annual, _ = self.fd.get_balance_sheet_annual(symbol) balance_quarterly, _ = self.fd.get_balance_sheet_quarterly(symbol) fundamentals['balance_annual'] = balance_annual fundamentals['balance_quarterly'] = balance_quarterly # Cash flow cashflow_annual, _ = self.fd.get_cash_flow_annual(symbol) cashflow_quarterly, _ = self.fd.get_cash_flow_quarterly(symbol) fundamentals['cashflow_annual'] = cashflow_annual fundamentals['cashflow_quarterly'] = cashflow_quarterly # Earnings earnings, _ = self.fd.get_earnings(symbol) fundamentals['earnings'] = earnings logger.info(f" Collected comprehensive fundamental data") return fundamentals except Exception as e: logger.error(f"Error getting fundamental data: {e}") return {} def get_economic_indicators(self) -> pd.DataFrame: """Get massive economic indicator data.""" try: logger.info(" Collecting economic indicators...") economic_data = {} if hasattr(self, 'fred'): # Federal Reserve Economic Data indicators = { 'GDP': 'GDP', 'INFLATION': 'CPIAUCSL', 'UNEMPLOYMENT': 'UNRATE', 'INTEREST_RATE': 'FEDFUNDS', 'MONEY_SUPPLY': 'M2SL', 'CONSUMER_SENTIMENT': 'UMCSENT', 'INDUSTRIAL_PRODUCTION': 'INDPRO', 'HOUSING_STARTS': 'HOUST', 'RETAIL_SALES': 'RSAFS', 'PAYROLLS': 'PAYEMS', 'YIELD_10Y': 'DGS10', 'YIELD_2Y': 'DGS2', 'DOLLAR_INDEX': 'DTWEXBGS', 'VIX': 'VIXCLS', 'OIL_PRICE': 'DCOILWTICO', 'GOLD_PRICE': 'GOLDAMGBD228NLBM' } for name, series_id in indicators.items(): try: data = self.fred.get_series(series_id, limit=2000) economic_data[name] = data logger.info(f" {name}: {len(data)} points") time.sleep(0.1) # Be nice to API except Exception as e: logger.warning(f" Failed to get {name}: {e}") # Convert to DataFrame if economic_data: econ_df = pd.DataFrame(economic_data) econ_df.index = pd.to_datetime(econ_df.index) logger.info(f" Economic indicators: {len(econ_df)} records") return econ_df return pd.DataFrame() except Exception as e: logger.error(f"Error getting economic indicators: {e}") return pd.DataFrame() def get_sector_etf_data(self, symbol: str) -> pd.DataFrame: """Get sector ETF data for correlation analysis.""" try: logger.info(f" Getting sector data for {symbol}") # Major sector ETFs sector_etfs = { 'SPY': 'S&P 500', 'QQQ': 'Nasdaq 100', 'XLK': 'Technology', 'XLF': 'Financial', 'XLE': 'Energy', 'XLV': 'Healthcare', 'XLI': 'Industrial', 'XLY': 'Consumer Discretionary', 'XLP': 'Consumer Staples', 'XLU': 'Utilities', 'XLB': 'Materials', 'XLRE': 'Real Estate', 'VIX': 'Volatility' } sector_data = {} for etf, name in sector_etfs.items(): try: ticker = yf.Ticker(etf) data = ticker.history(period="2y") if not data.empty: sector_data[f'{etf}_close'] = data['Close'] sector_data[f'{etf}_volume'] = data['Volume'] logger.info(f" {name} ({etf}): {len(data)} records") time.sleep(0.1) except Exception as e: logger.warning(f" Failed {etf}: {e}") if sector_data: sector_df = pd.DataFrame(sector_data) logger.info(f" Sector data: {len(sector_df)} records") return sector_df return pd.DataFrame() except Exception as e: logger.error(f"Error getting sector data: {e}") return pd.DataFrame() def get_options_data(self, symbol: str) -> Dict: """Get options data for volatility analysis.""" try: logger.info(f" Getting options data for {symbol}") ticker = yf.Ticker(symbol) # Get options chain options_dates = ticker.options options_data = {} for date in options_dates[:5]: # Get first 5 expiration dates try: chain = ticker.option_chain(date) options_data[date] = { 'calls': chain.calls, 'puts': chain.puts } logger.info(f" Options for {date}") time.sleep(0.1) except Exception as e: logger.warning(f" Failed options for {date}: {e}") return options_data except Exception as e: logger.error(f"Error getting options data: {e}") return {} def get_news_sentiment_data(self, symbol: str, days: int = 30) -> pd.DataFrame: """Get news sentiment data.""" try: logger.info(f" Getting news sentiment for {symbol}") # Get company info for search terms ticker = yf.Ticker(symbol) info = ticker.info company_name = info.get('longName', symbol) # NewsAPI newsapi_client = newsapi.NewsApiClient(api_key=self.news_key) # Get news articles from_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d') articles = newsapi_client.get_everything( q=f'"{company_name}" OR "{symbol}"', from_param=from_date, language='en', sort_by='relevancy', page_size=100 ) # Process articles for sentiment news_data = [] for article in articles['articles']: news_data.append({ 'date': article['publishedAt'], 'title': article['title'], 'description': article['description'], 'source': article['source']['name'], 'url': article['url'] }) if news_data: news_df = pd.DataFrame(news_data) news_df['date'] = pd.to_datetime(news_df['date']) # Add sentiment analysis (simplified) news_df['sentiment_score'] = self._analyze_sentiment(news_df) logger.info(f" News data: {len(news_df)} articles") return news_df return pd.DataFrame() except Exception as e: logger.error(f"Error getting news data: {e}") return pd.DataFrame() def _aggregate_intraday_to_daily(self, intraday_data: pd.DataFrame) -> pd.DataFrame: """Aggregate intraday data to daily with additional features.""" if intraday_data.empty: return pd.DataFrame() # Resample to daily daily = intraday_data.resample('D').agg({ 'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last', 'Volume': 'sum' }).dropna() # Add intraday features daily['intraday_volatility'] = intraday_data.groupby(intraday_data.index.date)['Close'].std() daily['intraday_range'] = (intraday_data.groupby(intraday_data.index.date)['High'].max() - intraday_data.groupby(intraday_data.index.date)['Low'].min()) daily['session_vwap'] = (intraday_data['Close'] * intraday_data['Volume']).groupby( intraday_data.index.date).sum() / intraday_data.groupby(intraday_data.index.date)['Volume'].sum() return daily def _combine_timeframes(self, daily, hourly, min15, min5) -> pd.DataFrame: """Combine multiple timeframe data.""" combined = daily.copy() # Add hourly features if not hourly.empty: hourly_features = hourly[['intraday_volatility', 'intraday_range', 'session_vwap']].add_suffix('_1h') combined = pd.concat([combined, hourly_features], axis=1) # Add 15min features if not min15.empty: min15_features = min15[['intraday_volatility', 'intraday_range', 'session_vwap']].add_suffix('_15m') combined = pd.concat([combined, min15_features], axis=1) # Add 5min features if not min5.empty: min5_features = min5[['intraday_volatility', 'intraday_range', 'session_vwap']].add_suffix('_5m') combined = pd.concat([combined, min5_features], axis=1) return combined.fillna(method='ffill') def _analyze_sentiment(self, news_df: pd.DataFrame) -> pd.Series: """Analyze sentiment of news articles (simplified).""" # This is a simplified version - in production, use proper NLP positive_words = ['gain', 'rise', 'profit', 'bullish', 'strong', 'growth', 'positive', 'beat', 'exceed'] negative_words = ['fall', 'drop', 'loss', 'bearish', 'weak', 'decline', 'negative', 'miss', 'disappoint'] sentiment_scores = [] for _, row in news_df.iterrows(): text = f"{row['title']} {row['description']}".lower() positive_count = sum(1 for word in positive_words if word in text) negative_count = sum(1 for word in negative_words if word in text) # Simple sentiment score sentiment = (positive_count - negative_count) / max(1, positive_count + negative_count) sentiment_scores.append(sentiment) return pd.Series(sentiment_scores)