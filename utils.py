""" Utility functions for the Neural Market Microstructure Predictor. """ import os import json import pickle import yaml import pandas as pd import numpy as np from datetime import datetime, timedelta from typing import Dict, List, Any, Optional, Union from loguru import logger import matplotlib.pyplot as plt import seaborn as sns def load_config(config_path: str = "config/config.yaml") -> Dict: """ Load configuration from YAML file. Args: config_path: Path to configuration file Returns: Configuration dictionary """ try: with open(config_path, 'r') as file: config = yaml.safe_load(file) logger.info(f"Configuration loaded from {config_path}") return config except Exception as e: logger.error(f"Error loading configuration: {e}") return {} def save_json(data: Dict, filepath: str): """ Save data to JSON file. Args: data: Data to save filepath: Output file path """ try: os.makedirs(os.path.dirname(filepath), exist_ok=True) with open(filepath, 'w') as f: json.dump(data, f, indent=2, default=str) logger.info(f"Data saved to {filepath}") except Exception as e: logger.error(f"Error saving JSON to {filepath}: {e}") def load_json(filepath: str) -> Dict: """ Load data from JSON file. Args: filepath: Input file path Returns: Loaded data """ try: with open(filepath, 'r') as f: data = json.load(f) logger.info(f"Data loaded from {filepath}") return data except Exception as e: logger.error(f"Error loading JSON from {filepath}: {e}") return {} def save_pickle(obj: Any, filepath: str): """ Save object to pickle file. Args: obj: Object to save filepath: Output file path """ try: os.makedirs(os.path.dirname(filepath), exist_ok=True) with open(filepath, 'wb') as f: pickle.dump(obj, f) logger.info(f"Object saved to {filepath}") except Exception as e: logger.error(f"Error saving pickle to {filepath}: {e}") def load_pickle(filepath: str) -> Any: """ Load object from pickle file. Args: filepath: Input file path Returns: Loaded object """ try: with open(filepath, 'rb') as f: obj = pickle.load(f) logger.info(f"Object loaded from {filepath}") return obj except Exception as e: logger.error(f"Error loading pickle from {filepath}: {e}") return None def setup_logging(log_level: str = "INFO", log_file: str = "logs/app.log"): """ Setup logging configuration. Args: log_level: Logging level log_file: Log file path """ try: os.makedirs(os.path.dirname(log_file), exist_ok=True) logger.remove() # Remove default handler # Add console handler logger.add( lambda msg: print(msg, end=""), level=log_level, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>" ) # Add file handler logger.add( log_file, level=log_level, format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}", rotation="10 MB", retention="30 days" ) logger.info("Logging setup completed") except Exception as e: print(f"Error setting up logging: {e}") def calculate_returns(prices: pd.Series, method: str = "simple") -> pd.Series: """ Calculate returns from price series. Args: prices: Price series method: 'simple' or 'log' Returns: Returns series """ if method == "simple": return prices.pct_change() elif method == "log": return np.log(prices / prices.shift(1)) else: raise ValueError("Method must be 'simple' or 'log'") def calculate_volatility(returns: pd.Series, window: int = 20) -> pd.Series: """ Calculate rolling volatility. Args: returns: Returns series window: Rolling window size Returns: Volatility series """ return returns.rolling(window=window).std() * np.sqrt(252) # Annualized def calculate_sharpe_ratio(returns: pd.Series, risk_free_rate: float = 0.02) -> float: """ Calculate Sharpe ratio. Args: returns: Returns series risk_free_rate: Risk-free rate (annualized) Returns: Sharpe ratio """ excess_returns = returns.mean() * 252 - risk_free_rate volatility = returns.std() * np.sqrt(252) return excess_returns / volatility if volatility != 0 else 0 def calculate_max_drawdown(prices: pd.Series) -> float: """ Calculate maximum drawdown. Args: prices: Price series Returns: Maximum drawdown (negative value) """ peak = prices.cummax() drawdown = (prices - peak) / peak return drawdown.min() def plot_predictions(actual: np.ndarray, predicted: np.ndarray, title: str = "Actual vs Predicted", figsize: tuple = (12, 6)): """ Plot actual vs predicted values. Args: actual: Actual values predicted: Predicted values title: Plot title figsize: Figure size """ plt.figure(figsize=figsize) plt.plot(actual, label='Actual', alpha=0.7) plt.plot(predicted, label='Predicted', alpha=0.7) plt.title(title) plt.xlabel('Time') plt.ylabel('Value') plt.legend() plt.grid(True) plt.tight_layout() plt.show() def plot_training_history(history: Dict, figsize: tuple = (15, 5)): """ Plot training history. Args: history: Training history dictionary figsize: Figure size """ fig, axes = plt.subplots(1, 3, figsize=figsize) # Loss axes[0].plot(history['loss'], label='Training Loss') if 'val_loss' in history: axes[0].plot(history['val_loss'], label='Validation Loss') axes[0].set_title('Model Loss') axes[0].set_xlabel('Epoch') axes[0].set_ylabel('Loss') axes[0].legend() axes[0].grid(True) # MAE if 'mae' in history: axes[1].plot(history['mae'], label='Training MAE') if 'val_mae' in history: axes[1].plot(history['val_mae'], label='Validation MAE') axes[1].set_title('Mean Absolute Error') axes[1].set_xlabel('Epoch') axes[1].set_ylabel('MAE') axes[1].legend() axes[1].grid(True) # MAPE if 'mape' in history: axes[2].plot(history['mape'], label='Training MAPE') if 'val_mape' in history: axes[2].plot(history['val_mape'], label='Validation MAPE') axes[2].set_title('Mean Absolute Percentage Error') axes[2].set_xlabel('Epoch') axes[2].set_ylabel('MAPE') axes[2].legend() axes[2].grid(True) plt.tight_layout() plt.show() def create_directory_structure(base_path: str = "."): """ Create the project directory structure. Args: base_path: Base path for the project """ directories = [ "config", "src/data", "src/models", "src/training", "src/prediction", "src/visualization", "notebooks", "tests", "scripts", "docker", "models", "logs", "data/raw", "data/processed" ] for directory in directories: path = os.path.join(base_path, directory) os.makedirs(path, exist_ok=True) logger.info(f"Created directory: {path}") def validate_data(data: pd.DataFrame, required_columns: List[str]) -> bool: """ Validate that data contains required columns. Args: data: DataFrame to validate required_columns: List of required column names Returns: True if valid, False otherwise """ missing_columns = [col for col in required_columns if col not in data.columns] if missing_columns: logger.error(f"Missing required columns: {missing_columns}") return False return True def format_currency(value: float, currency: str = "USD") -> str: """ Format value as currency. Args: value: Numeric value currency: Currency symbol Returns: Formatted currency string """ if currency == "USD": return f"${value:,.2f}" else: return f"{value:,.2f} {currency}" def calculate_correlation_matrix(data: pd.DataFrame, method: str = "pearson") -> pd.DataFrame: """ Calculate correlation matrix for numeric columns. Args: data: Input DataFrame method: Correlation method ('pearson', 'spearman', 'kendall') Returns: Correlation matrix """ numeric_data = data.select_dtypes(include=[np.number]) return numeric_data.corr(method=method) def plot_correlation_heatmap(corr_matrix: pd.DataFrame, figsize: tuple = (10, 8)): """ Plot correlation heatmap. Args: corr_matrix: Correlation matrix figsize: Figure size """ plt.figure(figsize=figsize) sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True, fmt='.2f', cbar_kws={"shrink": .8}) plt.title('Correlation Heatmap') plt.tight_layout() plt.show() if __name__ == "__main__": # Example usage setup_logging() logger.info("Utility functions loaded successfully")